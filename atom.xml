<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://yczzc.github.io</id>
    <title>yczzc</title>
    <updated>2020-04-18T09:43:18.707Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://yczzc.github.io"/>
    <link rel="self" href="https://yczzc.github.io/atom.xml"/>
    <logo>https://yczzc.github.io/images/avatar.png</logo>
    <icon>https://yczzc.github.io/favicon.ico</icon>
    <rights>All rights reserved 2020, yczzc</rights>
    <entry>
        <title type="html"><![CDATA[ConvLSTM PredRNN 和 PredRNN++]]></title>
        <id>https://yczzc.github.io/post/convlstm-predrnn-he-predrnn/</id>
        <link href="https://yczzc.github.io/post/convlstm-predrnn-he-predrnn/">
        </link>
        <updated>2020-04-18T08:29:28.000Z</updated>
        <content type="html"><![CDATA[<p><strong>Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting</strong><a href="https://arxiv.org/pdf/1506.04214.pdf">链接</a><br>
ConvLSTM是2015 NIPS上的论文，研究的问题是降雨预测。<br>
降雨预测问题是一个时空序列问题，与当前空间信息有关，也和历史信息有关；因此，作者用卷积运算代替Vanilla LSTM(FC-LSTM)中的普通点积；</p>
<p>FC-LSTM的公式化形式：<br>
<img src="https://yczzc.github.io/post-images/1587198913771.png" alt="" loading="lazy"></p>
<p>ConvLSTM的公式化形式（*表示卷积运算）：<br>
<img src="https://yczzc.github.io/post-images/1587198953111.png" alt="" loading="lazy"></p>
<p>此外，论文中采用Encoding-Forecasting Structure将多层ConvLSTM堆叠：<br>
<img src="https://yczzc.github.io/post-images/1587199798278.png" alt="" loading="lazy"><br>
在Encoding-Forecasting Structure中，预测网络的h和c取自编码网络最后一个状态；为了保证预测值和输入值维度相同，将预测网络所有状态concatenate后会有一个1*1卷积的操作。</p>
<p><strong>PredRNN: Recurrent Neural Networks for Predictive Learning using Spatiotemporal LSTMs</strong><a href="https://papers.nips.cc/paper/6689-predrnn-recurrent-neural-networks-for-predictive-learning-using-spatiotemporal-lstms.pdf">链接</a><br>
PredRNN是2017 NIPS的论文，提出了处理时空信息的 Spatiotemporal LSTM。<br>
作者认为时空记忆信息(memory states)不应该仅限于cell，提出用多层的结构学得memory<br>
states。</p>
<p>作者认为ConvLSTM中的多层结构编码了空间信息，但是导致每一层的cell相互独立，因此下一个时间点最底层ConvLSTM的输入不会考虑上一个时间点最高层ConvLSTM的输出。<br>
于是作者提出了加入spatiotemporal memory flow的ConvLSTM结构：<br>
<img src="https://yczzc.github.io/post-images/1587201494696.png" alt="" loading="lazy"></p>
<p>公式化形式为，其中的M为cell state，即c：<br>
<img src="https://yczzc.github.io/post-images/1587201564661.png" alt="" loading="lazy"></p>
<p>但是仅采用竖直方向上的spatiotemporal memory flow会损失时序连贯性(temporal<br>
coherency)并且容易梯度消失，于是作者提出了另外一种结构：<br>
<img src="https://yczzc.github.io/post-images/1587201891991.png" alt="" loading="lazy"><br>
公式化表示为：<br>
<img src="https://yczzc.github.io/post-images/1587202093263.png" alt="" loading="lazy"></p>
<p>这里可以看出M就是h和c的的结合。</p>
<p><strong>PredRNN++: Towards A Resolution of the Deep-in-Time Dilemma in Spatiotemporal Predictive</strong><a href="https://arxiv.org/pdf/1804.06300.pdf">链接</a><br>
PredRNN++是ICML 2018上的论文，对于前作PredRNN提出的Spatiotemporal LSTM结构进行了改进，提出Casual LSTM结构，并加入Gradient Highway Unit减缓梯度消失问题。<br>
<img src="https://yczzc.github.io/post-images/1587202701143.png" alt="" loading="lazy"><br>
<img src="https://yczzc.github.io/post-images/1587202727798.png" alt="" loading="lazy"></p>
<p>Gradient Highway Unit：<br>
<img src="https://yczzc.github.io/post-images/1587202904646.png" alt="" loading="lazy"></p>
<figure data-type="image" tabindex="1"><img src="https://yczzc.github.io/post-images/1587202942318.png" alt="" loading="lazy"></figure>
<p>参考：</p>
<ol>
<li>https://arxiv.org/pdf/1506.04214.pdf</li>
<li>https://papers.nips.cc/paper/6689-predrnn-recurrent-neural-networks-for-predictive-learning-using-spatiotemporal-lstms.pdf</li>
<li>https://arxiv.org/pdf/1804.06300.pdf</li>
</ol>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Capsule Network]]></title>
        <id>https://yczzc.github.io/post/capsule-network/</id>
        <link href="https://yczzc.github.io/post/capsule-network/">
        </link>
        <updated>2020-04-17T14:33:06.000Z</updated>
        <content type="html"><![CDATA[<p>Hinton最初在2011年就提出了胶囊网络概念，他认为CNN不符合生物神经系统，并且有很大的局限。</p>
<p>CNN能很好的理解局部信息，随着层数增加导致感受野增大，能够有很好的大局观。但是Hinton认为CNN是不太正确的，原因如下：</p>
<ol>
<li>生物视觉神经系统虽然是分层的，但是层数不多，远远低于现在常用CNN的层数；</li>
<li>在生物神经网络中，不会有BP机制；</li>
<li>人类会根据物体形状建立一种坐标框架(coordinate frame)，坐标框架作为一种先验知识，能够帮助人类理解不同方向的同一物体；而CNN的下采样丢弃了这一先验知识，从而只能处理不变性(Invariance)的问题（不变性是指物体本身不发生任何变化），无法处理同变性(Equivariance)的问题（同变性指物体可以发生变化，但变化后仍是这种物体）；我们希望CNN能够像人类一样理解图像，产生通用的图像表征，再用这个表征做任务驱动的任务，如分类、分割等等。而不是仅仅提高准确率。<br>
<img src="https://yczzc.github.io/post-images/1587135943123.png" alt="" loading="lazy"></li>
</ol>
<p>在哺乳动物大脑中存在大量皮层微柱(Cortical minicolumn)，其内部含有上百个神经元，并存在内部分层。这意味着人脑中的一层并不是类似现在NN的一层，而是有复杂的内部结构。于是Hinton提出了一个相应的结构，称为capsule，每一个capsule由多个神经元组成。</p>
<p><strong>Dynamic Routing Between Capsules (2017)</strong> <a href="https://arxiv.org/pdf/1710.09829v2.pdf">链接</a><br>
这篇论文中阐述，CapsuleNet输出向量的模长为出现的概率，因此采用squash函数作为激活函数，将输出向量的模长限制在[0, 1]；输出向量的方向表示图像性质实例化参数。<br>
算法步骤如下:（u表示上一层所有capsule的输出）<br>
<img src="https://yczzc.github.io/post-images/1587137648694.png" alt="" loading="lazy"><br>
<img src="https://yczzc.github.io/post-images/1587138556129.png" alt="" loading="lazy"></p>
<p><strong>Matrix Capsules with EM routing (2018)</strong><a href="https://openreview.net/pdf?id=HJWLfGWRb">链接</a><br>
未完……</p>
<p>参考：</p>
<ol>
<li>https://arxiv.org/pdf/1710.09829v2.pdf</li>
<li>https://www.jiqizhixin.com/articles/capsule-implement-sara-sabour-Feb02</li>
<li>https://zhuanlan.zhihu.com/p/29435406</li>
</ol>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[[论文] EGOCENTRIC SPATIAL MEMORY NETWORK]]></title>
        <id>https://yczzc.github.io/post/egocentric-spatial-memory-network/</id>
        <link href="https://yczzc.github.io/post/egocentric-spatial-memory-network/">
        </link>
        <updated>2020-04-17T00:45:07.000Z</updated>
        <content type="html"><![CDATA[<p>整理资料的时候发现了这篇很有意思的论文<a href="https://openreview.net/pdf?id=SkmM6M_pW">链接</a>。本文利用第一视角图像建模空间信息，估计智能体的运动信息并且建立一个自上而下的全局地图，完成导航任务。此外还利用了神经科学的研究成果，模拟人脑在建立全局图(top-down global maps)方面的工作原理。</p>
<p>论文首先介绍了神经科学的研究成果，人脑建立自上而下的全局地图需要四种细胞：head-direction cells(HDC), border and boundary vector cells(BVC), place cells(PC), grid cell(GC)，它们的功能分别是判别方向、判别边界、判别位置和将空间网格化(a grid cross the enviroment)。</p>
<p>基于人脑神经系统建立模型，模型包括Head Direction Unit(HDU), Boundary Vector Unit(BVU), Place Unit(PU), Grid Unit(GU)四部分。<br>
<img src="https://yczzc.github.io/post-images/1587086393761.png" alt="" loading="lazy"><br>
HDU利用2D-CNN在每一个时间点从动作空间学得运动信息，最小化分类损失函数。输入为RGB camera拍摄的图像。<br>
BVU通过2D-CNN和卷积逆置将第一视角图像转换成top-down局部地图。<br>
PU最小化分类损失函数，判别是否有loop closure。<br>
GU整合BVU生成的top-down局部地图，并记录所有经过的位置。在GU中，加入了门机制(gating mechanism)，用以判断是否陷入loop closure，降低了长期mapping产生的累计误差。<br>
<img src="https://yczzc.github.io/post-images/1587087326819.png" alt="" loading="lazy"></p>
<p>在训练时，分别单独训练每一个模块，训练好之后，将各个模块整合在一起并微调。</p>
<p>参考：</p>
<ol>
<li>https://openreview.net/pdf?id=SkmM6M_pW</li>
</ol>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[SinGAN  ConSinGAN 和 TuiGAN]]></title>
        <id>https://yczzc.github.io/post/singan-consingan-he-tuigan/</id>
        <link href="https://yczzc.github.io/post/singan-consingan-he-tuigan/">
        </link>
        <updated>2020-04-16T09:21:37.000Z</updated>
        <content type="html"><![CDATA[<p><strong>SinGAN</strong><br>
Technion和Google提出的&quot;SinGAN: Learning a Generative Model from a Single Natural Image&quot;（2019 ICCV的最佳论文）<a href="https://arxiv.org/pdf/1905.01164.pdf">论文链接</a>  <a href="https://www.youtube.com/watch?v=mdAcPe74tZI&amp;feature=youtu.be&amp;t=3217">视频链接</a>，是第一个拿单张图像进行训练的非条件生成网络。<br>
效果如下：（最左侧一列是训练图像，右侧几列是生成图像）<br>
<img src="https://yczzc.github.io/post-images/1587030102750.png" alt="" loading="lazy"><br>
SinGAN为多个GAN组成的金字塔结构。每一层的GAN是为了学到图像中的某一个patch，低层的GAN处理低分辨率的图像patch，patch size比较大，能快速理解图像；随着层数的增加，GAN处理更高分辨率的图像，patch size逐渐变小，逐渐学到图像的细节。这就是论文中提到的coarse-to-fine。<br>
此外，每一层的判别器并不将图像看作整体，通过这种方法， 它就可以知道真实的patch是什么样子。这样，生成器就可以通过生成在全局来看不同，但仅从patch来看却相似的图像，来达到“欺诈”的目的。<br>
对于第一个生成器，它的输入仅有随机噪声z。对于在更高分辨率上工作的生成器，它是将前一个生成器生成的图像作为输入，在此基础上生成比当前还要高分辨率的图像。所有的生成器都是单独训练的，也就是说在训练当前生成器时，所有以前的生成器的权重都保持不变。</p>
<figure data-type="image" tabindex="1"><img src="https://yczzc.github.io/post-images/1587030892081.png" alt="" loading="lazy"></figure>
<p><strong>ConSinGAN</strong><br>
Adobe和汉堡大学的研究人员发现，在某段时间内仅训练一个生成器会限制生成器之间的交互作用。因此，他们提出了同时训练多个生成器的ConSinGAN<a href="https://arxiv.org/pdf/2003.11512.pdf">论文链接</a>。效果图：<br>
<img src="https://yczzc.github.io/post-images/1587032979129.png" alt="" loading="lazy"><br>
为了防止过拟合，在某一个Stage时，模型只训练一部分生成器，而不是所有的生成器。此外，对于不同的生成器采用了不同的学习率。默认情况下，最多同时训练3个生成器，并对较低的生成器，分别将学习率调至0.1和0.001。<br>
<img src="https://yczzc.github.io/post-images/1587032532572.png" alt="" loading="lazy"></p>
<p><strong>TuiGAN</strong><br>
TuiGAN<a href="https://arxiv.xilesou.top/pdf/2004.04634.pdf">论文链接</a>研究的问题是图像转换，作者认为即便源域和目标域图像仅各有一幅图像也能完成转换任务。效果图：<br>
<img src="https://yczzc.github.io/post-images/1587034482162.png" alt="" loading="lazy"><br>
和之间两种模型类似，由多GAN组成的结构能学到有低分辨率图像到高分辨率图像的“渐进式转变”。最开始的GAN处理低分辨率图像，之后的生成器接收当前输入的本层图像和上一个小尺度生成器所生成图像的上采样的融合。<br>
<img src="https://yczzc.github.io/post-images/1587034342026.png" alt="" loading="lazy"><br>
在生成器中，利用一个attention注意力模块整合当前的本层图像和上一个小尺度生成器所生成的图像，完成不同尺度图像的融合。<br>
<img src="https://yczzc.github.io/post-images/1587034395040.png" alt="" loading="lazy"></p>
<p>参考：</p>
<ol>
<li>https://arxiv.org/pdf/1905.01164.pdf</li>
<li>https://arxiv.org/pdf/2003.11512.pdf</li>
<li>https://arxiv.xilesou.top/pdf/2004.04634.pdf</li>
<li>https://zhuanlan.zhihu.com/p/120846837</li>
<li>https://zhuanlan.zhihu.com/p/129045166</li>
</ol>
]]></content>
    </entry>
</feed>