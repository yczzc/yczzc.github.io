<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://yczzc.github.io</id>
    <title>yczzc</title>
    <updated>2020-04-17T02:05:00.694Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://yczzc.github.io"/>
    <link rel="self" href="https://yczzc.github.io/atom.xml"/>
    <logo>https://yczzc.github.io/images/avatar.png</logo>
    <icon>https://yczzc.github.io/favicon.ico</icon>
    <rights>All rights reserved 2020, yczzc</rights>
    <entry>
        <title type="html"><![CDATA[[论文] EGOCENTRIC SPATIAL MEMORY NETWORK]]></title>
        <id>https://yczzc.github.io/post/egocentric-spatial-memory-network/</id>
        <link href="https://yczzc.github.io/post/egocentric-spatial-memory-network/">
        </link>
        <updated>2020-04-17T00:45:07.000Z</updated>
        <content type="html"><![CDATA[<p>整理资料的时候发现了这篇很有意思的论文<a href="https://openreview.net/pdf?id=SkmM6M_pW">链接</a>。本文利用第一视角图像建模空间信息，估计智能体的运动信息并且建立一个自上而下的全局地图，完成导航任务。此外还利用了神经科学的研究成果，模拟人脑在建立全局图(top-down global maps)方面的工作原理。</p>
<p>论文首先介绍了神经科学的研究成果，人脑建立自上而下的全局地图需要四种细胞：head-direction cells(HDC), border and boundary vector cells(BVC), place cells(PC), grid cell(GC)，它们的功能分别是判别方向、判别边界、判别位置和将空间网格化(a grid cross the enviroment)。</p>
<p>基于人脑神经系统建立模型，模型包括Head Direction Unit(HDU), Boundary Vector Unit(BVU), Place Unit(PU), Grid Unit(GU)四部分。<br>
<img src="https://yczzc.github.io/post-images/1587086393761.png" alt="" loading="lazy"><br>
HDU利用2D-CNN在每一个时间点从动作空间学得运动信息，最小化分类损失函数。输入为RGB camera拍摄的图像。<br>
BVU通过2D-CNN和卷积逆置将第一视角图像转换成top-down局部地图。<br>
PU最小化分类损失函数，判别是否有loop closure。<br>
GU整合BVU生成的top-down局部地图，并记录所有经过的位置。在GU中，加入了门机制(gating mechanism)，用以判断是否陷入loop closure，降低了长期mapping产生的累计误差。<br>
<img src="https://yczzc.github.io/post-images/1587087326819.png" alt="" loading="lazy"></p>
<p>在训练时，分别单独训练每一个模块，训练好之后，将各个模块整合在一起并微调。</p>
<p>参考：</p>
<ol>
<li>https://openreview.net/pdf?id=SkmM6M_pW</li>
</ol>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[SinGAN  ConSinGAN 和 TuiGAN]]></title>
        <id>https://yczzc.github.io/post/singan-consingan-he-tuigan/</id>
        <link href="https://yczzc.github.io/post/singan-consingan-he-tuigan/">
        </link>
        <updated>2020-04-16T09:21:37.000Z</updated>
        <content type="html"><![CDATA[<p><strong>SinGAN</strong><br>
Technion和Google提出的&quot;SinGAN: Learning a Generative Model from a Single Natural Image&quot;（2019 ICCV的最佳论文）<a href="https://arxiv.org/pdf/1905.01164.pdf">论文链接</a>  <a href="https://www.youtube.com/watch?v=mdAcPe74tZI&amp;feature=youtu.be&amp;t=3217">视频链接</a>，是第一个拿单张图像进行训练的非条件生成网络。<br>
效果如下：（最左侧一列是训练图像，右侧几列是生成图像）<br>
<img src="https://yczzc.github.io/post-images/1587030102750.png" alt="" loading="lazy"><br>
SinGAN为多个GAN组成的金字塔结构。每一层的GAN是为了学到图像中的某一个patch，低层的GAN处理低分辨率的图像patch，patch size比较大，能快速理解图像；随着层数的增加，GAN处理更高分辨率的图像，patch size逐渐变小，逐渐学到图像的细节。这就是论文中提到的coarse-to-fine。<br>
此外，每一层的判别器并不将图像看作整体，通过这种方法， 它就可以知道真实的patch是什么样子。这样，生成器就可以通过生成在全局来看不同，但仅从patch来看却相似的图像，来达到“欺诈”的目的。<br>
对于第一个生成器，它的输入仅有随机噪声z。对于在更高分辨率上工作的生成器，它是将前一个生成器生成的图像作为输入，在此基础上生成比当前还要高分辨率的图像。所有的生成器都是单独训练的，也就是说在训练当前生成器时，所有以前的生成器的权重都保持不变。</p>
<figure data-type="image" tabindex="1"><img src="https://yczzc.github.io/post-images/1587030892081.png" alt="" loading="lazy"></figure>
<p><strong>ConSinGAN</strong><br>
Adobe和汉堡大学的研究人员发现，在某段时间内仅训练一个生成器会限制生成器之间的交互作用。因此，他们提出了同时训练多个生成器的ConSinGAN<a href="https://arxiv.org/pdf/2003.11512.pdf">论文链接</a>。效果图：<br>
<img src="https://yczzc.github.io/post-images/1587032979129.png" alt="" loading="lazy"><br>
为了防止过拟合，在某一个Stage时，模型只训练一部分生成器，而不是所有的生成器。此外，对于不同的生成器采用了不同的学习率。默认情况下，最多同时训练3个生成器，并对较低的生成器，分别将学习率调至0.1和0.001。<br>
<img src="https://yczzc.github.io/post-images/1587032532572.png" alt="" loading="lazy"></p>
<p><strong>TuiGAN</strong><br>
TuiGAN<a href="https://arxiv.xilesou.top/pdf/2004.04634.pdf">论文链接</a>研究的问题是图像转换，作者认为即便源域和目标域图像仅各有一幅图像也能完成转换任务。效果图：<br>
<img src="https://yczzc.github.io/post-images/1587034482162.png" alt="" loading="lazy"><br>
和之间两种模型类似，由多GAN组成的结构能学到有低分辨率图像到高分辨率图像的“渐进式转变”。最开始的GAN处理低分辨率图像，之后的生成器接收当前输入的本层图像和上一个小尺度生成器所生成图像的上采样的融合。<br>
<img src="https://yczzc.github.io/post-images/1587034342026.png" alt="" loading="lazy"><br>
在生成器中，利用一个attention注意力模块整合当前的本层图像和上一个小尺度生成器所生成的图像，完成不同尺度图像的融合。<br>
<img src="https://yczzc.github.io/post-images/1587034395040.png" alt="" loading="lazy"></p>
<p>参考：</p>
<ol>
<li>https://arxiv.org/pdf/1905.01164.pdf</li>
<li>https://arxiv.org/pdf/2003.11512.pdf</li>
<li>https://arxiv.xilesou.top/pdf/2004.04634.pdf</li>
<li>https://zhuanlan.zhihu.com/p/120846837</li>
<li>https://zhuanlan.zhihu.com/p/129045166</li>
</ol>
]]></content>
    </entry>
</feed>